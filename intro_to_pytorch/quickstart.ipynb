{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af6e22b",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "General rundown of the API for loading data and training a model.\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae28e5",
   "metadata": {},
   "source": [
    "## Working with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "727b6d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data first entry: (data, label):\n",
      "    sizes: img: torch.Size([1, 28, 28]) label: 9\n",
      "    types: img: <class 'torch.Tensor'> label: <class 'int'>\n",
      "    img_squeezed: torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x297f6b4f770>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "# Download training data\n",
    "# We are using the FashionMNIST dataset: https://github.com/zalandoresearch/fashion-mnist\n",
    "# This is a dataset of 28x28 grayscale images that are each associated with a label from 10 classes\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,  # Specify true to get the training data\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "# The FashionMNIST dataset is a dataset of\n",
    "# labelled images of clothing/shoes.\n",
    "#\n",
    "# Each datapoint is a PIL.Image\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,  # Specify false to get the test data\n",
    "    download=True,\n",
    "    transform=ToTensor(), # Converts the PIL.Image datapoints into Tensors.\n",
    ")\n",
    "\n",
    "display(training_data)\n",
    "display(test_data)\n",
    "\n",
    "print(f\"test_data first entry: (data, label):\")\n",
    "data: tuple[torch.Tensor, torch.Tensor] = test_data[0]\n",
    "img, label = data\n",
    "# Each image is in the format (C, H, W)\n",
    "# C = number of channels. It's 1 for all FashionMNIST datapoints, since they are all grayscale images\n",
    "# H = height of each image\n",
    "# W = width of each image\n",
    "print(f\"    sizes: img: {img.shape} label: {label}\")\n",
    "print(f\"    types: img: {type(img)} label: {type(label)}\")\n",
    "img_squeezed = img.squeeze()\n",
    "# We use squeeze to remove the from channel dimension, since we\n",
    "# need to feed a 2D matrix into plt's imshow \n",
    "print(f\"    img_squeezed: {img_squeezed.shape}\")\n",
    "plt.imshow(img_squeezed, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91ecc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# We cannot fetch an index of a dataloader, since it's an iterator\n",
    "# Therefore we have to iterate over it instead to access each element\n",
    "for X, y in test_dataloader:\n",
    "    # Since each image was (C, H< W), batching it adds another dimension N\n",
    "    # to the start, which represents the number of examples per batch\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    # Y is now a tensor of size N, which represents the labels associated\n",
    "    # with each example\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b9da0",
   "metadata": {},
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a08fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Since the input is a 2D matrix, we have to flatten it into a 1D vector\n",
    "        # that we can then feed into the neural network\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Since the input is a 2D matrix, we have to flatten it into a 1D vector\n",
    "        # that we can then feed into the neural network\n",
    "        x = self.flatten(x)\n",
    "        # In ML, logits is a another name for the raw (non-normalized) predictions\n",
    "        # that a classification model generates.\n",
    "        #  \n",
    "        # https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow/52111173\n",
    "        # https://developers.google.com/machine-learning/glossary/#logits\n",
    "        logits = self.linear_relu_stack(x)  \n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf96eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss (AKA Log-likelihood loss)\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "# We use the loss function like\n",
    "#   loss_fn(logits, ground_truth)\n",
    "# This applies softmax to the logits (normalizing the logits), and then applies log-likelihood\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Optimizer = How we calculate the gradient\n",
    "#   SGD = Stochastic gradient descent\n",
    "#       We estimate the gradient of the entire dataset by sampling \n",
    "#       a fixed number of points and identifying the gradient from the points.\n",
    "# lr = Learning rate\n",
    "# There are also different ways of calculating gradient to address osciliation issues, plataeus, etc.\n",
    "#   See Adagrad, RMSProp, etc.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65033194",
   "metadata": {},
   "source": [
    "## Optimizing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad44a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() # Configure model for training, which tells dropout layers to perform dropout\n",
    "    # Iterate over the batches\n",
    "    #\n",
    "    # The dataloader loads a dataset and splits it into batches so we\n",
    "    # can perform stochastic gradient descent. It returns\n",
    "    # an iterator over the batches.\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Move the X and Y tensors to the device we're running the model on\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X) # Returns a tensor that contains the predictions from our model\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Perform backpropagation from the loss function\n",
    "        loss.backward()\n",
    "        optimizer.step() # Updates the parameters\n",
    "        # Reset the accumulated gradient after we're done calculating the gradients. \n",
    "        # Remember, in backpropagation we accrue gradient values, which is important \n",
    "        # for RNNs that repeatedly feed into itself.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # Print the loss every once in a while so we can track how our training is doing\n",
    "        if batch % 100 == 0:\n",
    "            # batch = current batch number\n",
    "            # len(X) = number of samples per batch (This is fixed across al batches)\n",
    "            # \n",
    "            # By multiplying batch size by total \n",
    "            _loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            # NOTE: The loss is a scalar value (regular number)\n",
    "            # Scalars in pytorch are represented as tensors of size torch.size([]), which are zero dimensional\n",
    "            # \n",
    "            # .item() extracts the scalar value from the zero dimensional tensor\n",
    "            print(f\"loss: {loss.shape} {loss} _loss: {_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            if DEBUG:\n",
    "                print(f\"types: pred: {pred[:3]} y: {y[:3]} loss: {loss}\")\n",
    "\n",
    "# train(train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03a3712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader: DataLoader, model: nn.Module, loss_fn: nn.Module):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # Sets module to evaluation mode, which is important for telling dropout layers to stop dropping out\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): # Disables autograd engine\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred: torch.Tensor = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches # Calculate average test_loss across all batches\n",
    "    correct /= size # Calculate correct %\n",
    "    print(f\"Test error: \\n Accuracy: {(100 * correct):>0.1f}% Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "# test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30747481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: torch.Size([]) 2.3049347400665283 _loss: 2.304935  [   64/60000]\n",
      "loss: torch.Size([]) 2.2955398559570312 _loss: 2.295540  [ 6464/60000]\n",
      "loss: torch.Size([]) 2.2732653617858887 _loss: 2.273265  [12864/60000]\n",
      "loss: torch.Size([]) 2.269732713699341 _loss: 2.269733  [19264/60000]\n",
      "loss: torch.Size([]) 2.2397804260253906 _loss: 2.239780  [25664/60000]\n",
      "loss: torch.Size([]) 2.2150111198425293 _loss: 2.215011  [32064/60000]\n",
      "loss: torch.Size([]) 2.2194411754608154 _loss: 2.219441  [38464/60000]\n",
      "loss: torch.Size([]) 2.183429479598999 _loss: 2.183429  [44864/60000]\n",
      "loss: torch.Size([]) 2.1880457401275635 _loss: 2.188046  [51264/60000]\n",
      "loss: torch.Size([]) 2.1558046340942383 _loss: 2.155805  [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 45.0% Avg loss: 2.149861 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: torch.Size([]) 2.1584279537200928 _loss: 2.158428  [   64/60000]\n",
      "loss: torch.Size([]) 2.150747060775757 _loss: 2.150747  [ 6464/60000]\n",
      "loss: torch.Size([]) 2.0961079597473145 _loss: 2.096108  [12864/60000]\n",
      "loss: torch.Size([]) 2.1229381561279297 _loss: 2.122938  [19264/60000]\n",
      "loss: torch.Size([]) 2.0545055866241455 _loss: 2.054506  [25664/60000]\n",
      "loss: torch.Size([]) 2.001059055328369 _loss: 2.001059  [32064/60000]\n",
      "loss: torch.Size([]) 2.0292205810546875 _loss: 2.029221  [38464/60000]\n",
      "loss: torch.Size([]) 1.9520424604415894 _loss: 1.952042  [44864/60000]\n",
      "loss: torch.Size([]) 1.967475175857544 _loss: 1.967475  [51264/60000]\n",
      "loss: torch.Size([]) 1.8933546543121338 _loss: 1.893355  [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 56.3% Avg loss: 1.892453 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: torch.Size([]) 1.9216018915176392 _loss: 1.921602  [   64/60000]\n",
      "loss: torch.Size([]) 1.889087438583374 _loss: 1.889087  [ 6464/60000]\n",
      "loss: torch.Size([]) 1.7832298278808594 _loss: 1.783230  [12864/60000]\n",
      "loss: torch.Size([]) 1.835031270980835 _loss: 1.835031  [19264/60000]\n",
      "loss: torch.Size([]) 1.7050880193710327 _loss: 1.705088  [25664/60000]\n",
      "loss: torch.Size([]) 1.667091965675354 _loss: 1.667092  [32064/60000]\n",
      "loss: torch.Size([]) 1.6852415800094604 _loss: 1.685242  [38464/60000]\n",
      "loss: torch.Size([]) 1.5945336818695068 _loss: 1.594534  [44864/60000]\n",
      "loss: torch.Size([]) 1.617550253868103 _loss: 1.617550  [51264/60000]\n",
      "loss: torch.Size([]) 1.511660099029541 _loss: 1.511660  [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 62.5% Avg loss: 1.533323 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: torch.Size([]) 1.5956932306289673 _loss: 1.595693  [   64/60000]\n",
      "loss: torch.Size([]) 1.5560153722763062 _loss: 1.556015  [ 6464/60000]\n",
      "loss: torch.Size([]) 1.4178369045257568 _loss: 1.417837  [12864/60000]\n",
      "loss: torch.Size([]) 1.4952263832092285 _loss: 1.495226  [19264/60000]\n",
      "loss: torch.Size([]) 1.3573739528656006 _loss: 1.357374  [25664/60000]\n",
      "loss: torch.Size([]) 1.364436149597168 _loss: 1.364436  [32064/60000]\n",
      "loss: torch.Size([]) 1.372664451599121 _loss: 1.372664  [38464/60000]\n",
      "loss: torch.Size([]) 1.3060702085494995 _loss: 1.306070  [44864/60000]\n",
      "loss: torch.Size([]) 1.3349487781524658 _loss: 1.334949  [51264/60000]\n",
      "loss: torch.Size([]) 1.2353801727294922 _loss: 1.235380  [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 63.4% Avg loss: 1.264844 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: torch.Size([]) 1.3421374559402466 _loss: 1.342137  [   64/60000]\n",
      "loss: torch.Size([]) 1.3183255195617676 _loss: 1.318326  [ 6464/60000]\n",
      "loss: torch.Size([]) 1.160323143005371 _loss: 1.160323  [12864/60000]\n",
      "loss: torch.Size([]) 1.270394206047058 _loss: 1.270394  [19264/60000]\n",
      "loss: torch.Size([]) 1.1269813776016235 _loss: 1.126981  [25664/60000]\n",
      "loss: torch.Size([]) 1.161618947982788 _loss: 1.161619  [32064/60000]\n",
      "loss: torch.Size([]) 1.1800847053527832 _loss: 1.180085  [38464/60000]\n",
      "loss: torch.Size([]) 1.125082015991211 _loss: 1.125082  [44864/60000]\n",
      "loss: torch.Size([]) 1.1587318181991577 _loss: 1.158732  [51264/60000]\n",
      "loss: torch.Size([]) 1.0738680362701416 _loss: 1.073868  [57664/60000]\n",
      "Test error: \n",
      " Accuracy: 64.6% Avg loss: 1.097192 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21516856",
   "metadata": {},
   "source": [
    "## Saving + Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3d2cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save our model's internal state (which is a python dictionary), into a file\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96d31049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading a model from a file\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f9a3674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n",
      "Predicted: \"Pullover\", Actual: \"Pullover\"\n",
      "Predicted: \"Trouser\", Actual: \"Trouser\"\n",
      "Predicted: \"Trouser\", Actual: \"Trouser\"\n",
      "Predicted: \"Pullover\", Actual: \"Shirt\"\n",
      "Predicted: \"Trouser\", Actual: \"Trouser\"\n",
      "Predicted: \"Coat\", Actual: \"Coat\"\n",
      "Predicted: \"Coat\", Actual: \"Shirt\"\n",
      "Predicted: \"Sneaker\", Actual: \"Sandal\"\n",
      "Predicted: \"Sneaker\", Actual: \"Sneaker\"\n"
     ]
    }
   ],
   "source": [
    "# Using our new model to make predictions\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "# Evaluate first 10 test datapoints\n",
    "for i in range(10):\n",
    "    x, y = test_data[i]\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        pred = model(x)\n",
    "        predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "        print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
